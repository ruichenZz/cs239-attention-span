# Evaluation

## Conducting user evaluation [+5]

Your goal is to assess the usability of your system.

1. [+5] Evaluate your system with at least 10 participants. Write and submit 0.5-1p of notes for each participant.
    
    Participant 1: 
    
    Participant 2: 
    
    Participant 3: 
    
    Participant 4: 
    
    Participant 5: 
    
    Participant 6: 
    
    Participant 7: 
    
    Participant 8: 
    
    Participant 9: 
    
    Participant 10: 
    
2. [+1] DEPTH: Evaluate with 5 more participants. Feel free to make changes to your system between the first and second round of evaluation. If you do make changes, summarize the changes you made and why.

## After your evaluation [+4]

1. [+4] Analyze your data and write up your key findings. The findings should be about 0.5-1p for each motivating question and any other interesting findings.
- For any qualitative data where you cannot easily remember the details of the results, a thematic analysis is required. When you conduct a thematic analysis, include your codebook.
- For any quantitative data, submit a script for analysis + your data. Recommendation: Create a notebook for your analysis. Someone should be able to run your notebook to reproduce your results.

###Qualitative Data:

1. Usefulness and Actionability

Overall Impression: Participants generally found the system useful and practical in helping them refine their scripts. The combination of visualizations and suggestions provided structured guidance, making it easier to identify areas for improvement.

Key Benefits:
- Visualization: Helped participants quickly spot sections that needed more elements be inserted. The color-coded timeline was particularly effective in highlighting information-dense areas that might be overwhelming for viewers. Helped identify where to add content, preventing gaps that might be noticed during video editing.
- Suggestions: Provided actionable insights, such as where to insert images or storytelling elements. Though one would not completely follow the suggestions, they inspires new idea about what to change.
  
Example Feedback:
"The suggestions are very detailed and even if I don’t follow them exactly, they inspire new ideas of how to improve my script."
"I liked that the suggestions weren’t vague like ‘make it more engaging’—they actually told me what to put there."

2. Customization and Personalization

Issue: While the system’s suggestions were generally helpful, some participants felt they didn’t fully align with their individual creative styles.

Main Concern:
- The system sometimes suggested changes that didn’t match a participant’s preferred storytelling tone or content structure.
- Some felt that while the suggestions worked for a general audience, they lacked nuance for more experienced creators with established styles.

Example Feedback:
"It’s useful, but I wouldn’t use it all the time because my content has a very specific style that the system doesn’t fully get."

3. Long-Term Use Potential and Applicability Beyond Content Creation

Usage Frequency:
- Most participants said they would use the system occasionally rather than for every script.
- They saw it as a helpful tool rather than something they would rely on consistently.

Potential in Other Domains:
- Some participants, especially those who were not content creators, noted that the system could be useful for other types of scriptwriting, such as speech preparations. The structured approach to suggestions and visualizations was seen as valuable beyond just video content.

Example Feedback:
"I wouldn’t use it for every script, but I’d definitely check it for ones where I feel stuck or unsure."
"... this could actually be useful for presentations, not just video scripts."

4. User Interaction and Learning Curve

Ease of Use: Most participants felt the system was intuitive and easy to navigate, but there could be some improvements on adding more headings or showing instructions.

Example Feedback:
"I got how to use it pretty quickly, but there could be a heading on each text box so that they are easier to be reviewed."


##Quantitative Data:
(graph see the jupyter notebook)

- Ease of Use: Participants were able to find out how to use the system relatively quickly.
- Visualization Effectiveness: The high number of interactions (6) in the visualization phase suggests that users frequently referred back and forth between the visualization and script, indicating its usefulness in structuring content. The color-coded timeline effectively helped participants understand content density and flow. Interestingly, more than one participant reacted to the visualizations when first seeing them, attracted by the high contrast and vibrant colors. Some even looked back at them multiple times just because they were so eye-catching.
- Suggestions Engagement: Users spent the most time reading the suggestions (170 seconds), indicating that they found them detailed and actionable. The lower number of interactions (3) suggests that users were deliberate in reviewing them rather than frequently switching back and forth.
- Editing Effort: The longest time was spent on making edits (210 seconds), as users applied insights from both suggestions and visualizations. After making changes, some participants generated new visualizations and suggestions, noticing improvements in their script. This gave them a sense of accomplishment, as they could visibly track their progress and see how their refinements made a difference.
- Overall Summary: The system is useful and guides users effectively through content improvement. While users engaged heavily with visualizations for structure and content balance, they also found suggestions actionable for refining their script. The workflow appears intuitive and supportive of content revision.



### Group Reflection [+1]

_1. What is one thing that went well in your evaluation?_

The system did not malfunction throughout the whole evaluation (kind of unexpected but great!).

_2. What is one thing that you wish you could have done differently?_

If we had more time, we could have used eye-tracking or cursor position tracking to measure how often participants switched between the script and the suggestions/visualization. This would provide a more precise count, as manually tracking while timing was difficult.

_3. How, if at all, did your participants represent the personas you intended to design for?_

Not all participants matched our intended persona—most were content creators, but some were not. However, this didn’t cause major issues.

_4. How do you think this impacted your results?_

The non-content creators provided feedback from an audience perspective, which is also valuable. They weren’t limited by a creator’s mindset and confirmed that our suggestions made content more engaging.

_5. Based on the above, what does this say about the potential applicability of your system?_

Our system seems highly applicable—several participants mentioned they would use it. Some non-content creators even suggested it could work for other scenarios, like speech script checking where audience engagement and information density also need to be balanced.

_6. What new questions do you have based on your evaluation?_

- How does the system’s impact vary across different content styles and platforms?
- Can it also work on short videos?


Did you use a generative AI tool for this assignment? If so, which tool(s) and how?

How much time did you spend on this assignment as a group? individually?

Zihan Jiang: 6 hr
