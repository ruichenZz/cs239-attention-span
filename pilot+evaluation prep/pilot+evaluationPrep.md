Pilot [+1]

[+0.5] Present the pilot user with a brief statement of the scenario and task. Ask the pilot user to complete the task. Note: You might feel (very) nervous that something will break. That is OK. It's ok for the pilot user to break things as they test out your system. Be prepared to restart/recover your system when things break. Note what happened step by step. Include 0.5-1p of notes on one pilot user. Additionally, summarize in a few sentences: What happened? Why? What changes do you need to make to your system before the next pilot?

[+0.5] Involve another pilot user outside of the course. Include 0.5-1p of notes on this second pilot user. Summarize in a few sentences: What happened? Why? What changes do you need to make to your system before the next pilot?


Before conducting an evaluation [+3]

[+0.5] Articulate1-2 questions motivating the evaluation. In other words, what are the 1-2 things you want to prioritize learning through the evaluation?

How useful and actionable do users find the suggestions and visualizations?
 

[+0.5] What metrics will you use to answer the above research questions? Why are these metrics appropriate? What are the benefits and drawbacks of using these metrics?
Requirements: You are required to conduct a mixed-methods study where you collect qualitative and quantitative data. In your response to this question, describe what kind of data (e.g., open-ended survey, interview, time, clicks, etc.) will be useful for answering your motivating questions. 

**Quantitative Metrics**
1. Time spent per task: How long users spend on different stages:
- Initial exploration (how long users take to understand how to use the website before interacting with suggestions and visualizations)
- Reading the suggestions
- Looking at the visualizations
- Making edits based on suggestions
- Completing the script revision cycle 

2. Number of interactions:
- Clicks on suggestions -> How many suggestions were accepted, modified, or ignored?
- How often users switch between text and visualization? -> Number of edits made after seeing visualization.

* *Benefits:* *
- Helps identify points where user is confused.
- Provides objective insights into user behavior and can be statistically analyzed for trends.

* *Drawbacks:* *
Time data alone doesn’t reveal why users behave a certain way, and users might hesitate, which would affect timing accuracy.


**Qualitative Metrics**
Methods:
1. Post-task survey (Likert scale)
- "How useful were the suggestions?" (1-5 scale)
- "How actionable were the visualizations?" (1-5 scale)
2. Short user interviews (~4 min)
- Why they did/didn’t follow suggestions?
- What would improve the visualization/suggestions?

* *Benefits:* *
- Provides explanatory insights into user decisions.
- Helps refine future design based on user pain points.
- Captures details that numbers miss (e.g., "I ignored the suggestion because…").

* *Drawbacks:* *
- Harder to analyze than raw numbers.
- Responses may be biased with different users or incomplete because of various reasons.



[+1] Specify a plan for recruiting participants.

* *How will you contact participants (e.g., mailing lists, in-person, etc)?* *

Participants will be recruited through personal networks and mailing lists within the video creator community. Specifically:
Outreach will be done through friends and contacts who are long video creators. Creators will be invited via mailing lists and direct messages.

* *What are your inclusion/exclusion criteria for participants? * *

**Inclusion:**
- Long video creators who write scripts or plan to improve their scripting process.
- Those interested in adapting long-form scripts into short-form content.
**Exclusion:**
- Creators who do not write scripts at all.
- Creators who focus exclusively on short and improvised content.
- People who are not content creators at all.

* *Will you include participants you interviewed for user research? Why or why not?* *

**We will include user research participants:**
Pros
- Comparative Insights: They can evaluate whether the tool matches their previous scripting habits.
- They can comment on whether the tool addresses the challenges they previously identified.

Cons
- Potential Bias: Their familiarity may make them overlook first-time usability issues. However, this bias is limited since they are also experiencing the tool for the first time.

**So we will** add another question for user research participants:
- "Compared to the needs you previously described, does this tool successfully address your pain points? Why or why not?"


* *Where will you perform the evaluation?* *

Remote evaluation via Zoom screen sharing. Participants will control the researcher’s screen, so they do not need to install anything.

* *What data will you collect from participants? How will you inform them of this and obtain informed consent?* *
- Quantitative Data: Time spent and number of interactions as mentioned in metrics before.
- Qualitative Data: Post-task survey and short interview as mentioned.

Before the session, participants will receive a message/email explaining:
1. The purpose of the evaluation - helping improve scriptwriting tools for video creators.
2. The tasks they will perform - testing the tool and providing feedback.
3. The types of data collected - interaction time, survey responses, and some insights through interview.
4. That participation is voluntary, and they can opt out at any time.

Consent Process:
- A verbal consent statement will be read before the session begins.
- Participants will confirm their consent before proceeding.
- No personal identifiable information will be recorded.


 

[+1] Write out a step-by-step protocol for conducting each user evaluation. Getting on the same page is important for more easily conducting studies and analyzing data across participants. Your protocol should include: (1) a script of what you will say to each participant; (2) what behaviors/responses you expect from participants and how that may change the flow of the study, if at all; and (3) how you will transition between phases of the study (e.g., from a task to an interview). 

For fun [+0.5]
[+0.5] Name your system!
ScriptPulse

[+0.5] DEPTH: Design a logo for your system. Include a PNG in your repo. Add it to the README. 
 

Did you use a generative AI tool for this assignment? If so, which tool(s) and how?

 

How much time did you spend on this assignment

- as a group?

- individually?
